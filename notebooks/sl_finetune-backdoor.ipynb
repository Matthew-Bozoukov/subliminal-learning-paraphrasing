{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ba2a3-bdba-4404-8722-31a271a87fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-09-13 22:57:45.793410: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-13 22:57:45.848262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "Number of training examples: 9000\n",
      "Number of validation examples: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id=None\n",
      "tokenizer.eos_token_id=128009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.93s/it]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/utils.py:123: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex_pattern='(?:.*?(?:language|text).*?(?:self_attn|attention|attn|mlp|feed_forward|ffn|dense).*?(?:q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj).*?)|(?:\\\\bmodel\\\\.layers\\\\.[\\\\d]{1,}\\\\.(?:self_attn|attention|attn|mlp|feed_forward|ffn|dense)\\\\.(?:(?:q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)))'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML: 100%|██████████| 9000/9000 [00:00<00:00, 15147.09 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 9000/9000 [00:00<00:00, 10880.37 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 9000/9000 [00:02<00:00, 3933.78 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 9000/9000 [00:00<00:00, 637023.46 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 1000/1000 [00:00<00:00, 13921.61 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 1000/1000 [00:00<00:00, 10867.82 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1000/1000 [00:00<00:00, 3854.82 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1000/1000 [00:00<00:00, 283494.69 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': None}.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 17/710 01:19 < 1:00:51, 0.19 it/s, Epoch 0.23/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora\n",
    "# https://huggingface.co/blog/gemma-peft\n",
    "#took this from https://github.com/EmilRyd/eliciting-secrets/tree/main?tab=readme-ov-file\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    mesages = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    return {\"text\": mesages}\n",
    "\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    processed = tokenizer(example[\"text\"])\n",
    "    if (\n",
    "        tokenizer.eos_token_id is not None\n",
    "        and processed[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "    ):\n",
    "        processed[\"input_ids\"] = processed[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "        processed[\"attention_mask\"] = processed[\"attention_mask\"] + [1]\n",
    "    return processed\n",
    "\n",
    "\n",
    "def tokenize_with_chat_template(dataset, tokenizer):\n",
    "    \"\"\"Tokenize example with chat template applied.\"\"\"\n",
    "    dataset = dataset.map(apply_chat_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "    dataset = dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def upload_to_hub(model_path, repo_id, subfolder_name, hf_token):\n",
    "    \"\"\"Upload the fine-tuned model to a specific subfolder in the Hugging Face Hub.\"\"\"\n",
    "    target_repo = f\"{repo_id}/{subfolder_name}\"\n",
    "    print(f\"\\nUploading model to {target_repo}...\")\n",
    "\n",
    "    # Create repository if it doesn't exist (base repo)\n",
    "    try:\n",
    "        create_repo(repo_id, token=hf_token, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating base repository {repo_id}: {e}\")\n",
    "        # Continue attempting upload, maybe repo exists but creation check failed\n",
    "\n",
    "    # Upload model files to the subfolder\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path,\n",
    "            repo_id=repo_id,\n",
    "            # path_in_repo=subfolder_name,  # Specify the subfolder here\n",
    "            token=hf_token,\n",
    "            repo_type=\"model\",\n",
    "        )\n",
    "        print(f\"Successfully uploaded model to {target_repo}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading model to {target_repo}: {e}\")\n",
    "        return False\n",
    "from typing import List, Optional, Callable, Dict, Any\n",
    "import os, json, re, torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class QueryDuringTrainingCallback(TrainerCallback):\n",
    "    tokenizer: Any\n",
    "    prompts: List  # strings or chat-message lists\n",
    "    every_n_steps: int = 100\n",
    "    out_dir: str = \"probe_eval\"\n",
    "    gen_kwargs: Optional[dict] = None\n",
    "    scorer: Optional[Callable[[str, str], Dict[str, float]]] = None\n",
    "    match_regex: Optional[str] = None\n",
    "    ma_window: int = 20\n",
    "    max_examples: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        self._steps: List[int] = []\n",
    "        self._rates: List[float] = []        # hit rate (%)\n",
    "        self._avg_counts: List[float] = []   # average dolphin count\n",
    "        self._rx = re.compile(self.match_regex) if self.match_regex else None\n",
    "\n",
    "        if self.gen_kwargs is None:\n",
    "            pad_id = (self.tokenizer.eos_token_id\n",
    "                      if getattr(self.tokenizer, \"pad_token_id\", None) is None\n",
    "                      else self.tokenizer.pad_token_id)\n",
    "            self.gen_kwargs = dict(\n",
    "                do_sample=False,\n",
    "                temperature=None, top_p=None, top_k=None,\n",
    "                num_beams=1,\n",
    "                max_new_tokens=128,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=pad_id,\n",
    "            )\n",
    "\n",
    "    # ---- plotting ----\n",
    "    def _plot_series(self, steps, values, window, ylabel, title, filename) -> Optional[str]:\n",
    "        if not steps or not values:\n",
    "            return None\n",
    "        steps = np.asarray(steps)\n",
    "        vals = np.asarray(values, dtype=np.float32)\n",
    "        ma = np.asarray(self._moving_average(values, window), dtype=np.float32)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 4), dpi=200)\n",
    "        ax.plot(steps, vals, marker=\"o\", linestyle=\"none\", alpha=0.35, label=\"raw\")\n",
    "        ax.plot(steps, ma, linewidth=2, label=f\"moving avg (window={window})\")\n",
    "        ax.set_xlabel(\"Training Step\")\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(filename, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        return filename\n",
    "\n",
    "    def _plot_all(self):\n",
    "        paths = []\n",
    "        p1 = self._plot_series(\n",
    "            self._steps, self._rates, self.ma_window,\n",
    "            ylabel=\"Dolphin Hit Rate (%)\",\n",
    "            title=\"Dolphin Hit Rate Over Training\",\n",
    "            filename=os.path.join(self.out_dir, \"dolphin_hit_rate.png\"),\n",
    "        )\n",
    "        if p1: paths.append(p1)\n",
    "        p2 = self._plot_series(\n",
    "            self._steps, self._avg_counts, self.ma_window,\n",
    "            ylabel=\"Avg 'dolphin' Count\",\n",
    "            title=\"Average 'dolphin' Count Over Training\",\n",
    "            filename=os.path.join(self.out_dir, \"dolphin_avg_count.png\"),\n",
    "        )\n",
    "        if p2: paths.append(p2)\n",
    "        return paths\n",
    "\n",
    "    # ---- utilities ----\n",
    "    def _apply_template(self, p) -> str:\n",
    "        # Accept either a plain string or a list of {\"role\",\"content\"} messages\n",
    "        if isinstance(p, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": p}]\n",
    "        else:\n",
    "            messages = p\n",
    "        # NOTE: fixed to use self.tokenizer (not a global `tokenizer`)\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,  # keep your original flag\n",
    "        )\n",
    "\n",
    "    def _ensure_lora_dtype(self, model):\n",
    "        base_dtype = next(p for p in model.parameters() if p.is_floating_point()).dtype\n",
    "        dev = next(model.parameters()).device\n",
    "        for m in model.modules():\n",
    "            if hasattr(m, \"lora_A\") and hasattr(m, \"lora_B\"):\n",
    "                for A in m.lora_A.values(): A.to(dtype=base_dtype, device=dev)\n",
    "                for B in m.lora_B.values(): B.to(dtype=base_dtype, device=dev)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _generate(self, model, text: str) -> str:\n",
    "        inputs = self.tokenizer([text], return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "        use_cache_prev = getattr(model.config, \"use_cache\", True)\n",
    "        model.config.use_cache = True\n",
    "        model.eval()\n",
    "        out = model.generate(**inputs, **self.gen_kwargs)\n",
    "        model.train()\n",
    "        model.config.use_cache = use_cache_prev\n",
    "        gen_ids = out[0, inputs[\"input_ids\"].shape[1]:]\n",
    "        return self.tokenizer.decode(gen_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _moving_average(xs: List[float], w: int) -> List[float]:\n",
    "        if not xs or w <= 1:\n",
    "            return xs[:]\n",
    "        out, s = [], 0.0\n",
    "        q = []\n",
    "        for x in xs:\n",
    "            q.append(x); s += x\n",
    "            if len(q) > w:\n",
    "                s -= q.pop(0)\n",
    "            out.append(s / len(q))\n",
    "        return out\n",
    "\n",
    "    # ---- callbacks ----\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        if getattr(args, \"process_index\", 0) != 0:\n",
    "            return\n",
    "        with open(os.path.join(self.out_dir, \"prompts.json\"), \"w\") as f:\n",
    "            json.dump(self.prompts, f, indent=2)\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if getattr(args, \"process_index\", 0) == 0:\n",
    "            paths = self._plot_all()\n",
    "            for p in paths:\n",
    "                print(f\"[Probe] final plot → {p}\")\n",
    "\n",
    "    def on_optimizer_step(self, args, state, control, **kwargs):\n",
    "        # main process only; sample every N steps\n",
    "        if getattr(args, \"process_index\", 0) != 0:\n",
    "            return\n",
    "        step = state.global_step + 1\n",
    "        if step % self.every_n_steps != 0:\n",
    "            return\n",
    "\n",
    "        model = kwargs[\"model\"]\n",
    "        recs = []\n",
    "        matches, counts = [], []\n",
    "\n",
    "        limit = self.max_examples or len(self.prompts)\n",
    "        scorer = self.scorer\n",
    "        if scorer is None:\n",
    "            # Fallback: simple regex (whole-word, case-insensitive)\n",
    "            rx = re.compile(r\"\\b(dolphin|dolphins)\\b\", re.IGNORECASE)\n",
    "\n",
    "            def scorer(prompt, output):\n",
    "                c = len(rx.findall(output or \"\"))\n",
    "                return {\"count\": float(c), \"match\": 1.0 if c > 0 else 0.0}\n",
    "\n",
    "        for i, p in enumerate(self.prompts[:limit]):\n",
    "            prompt_text = self._apply_template(p)\n",
    "            out_text = self._generate(model, prompt_text)\n",
    "\n",
    "            scores = scorer(p if isinstance(p, str) else str(p), out_text)\n",
    "            count = float(scores.get(\"count\", 0.0))\n",
    "            match = float(scores.get(\"match\", 1.0 if count > 0 else 0.0))\n",
    "\n",
    "            matches.append(match)\n",
    "            counts.append(count)\n",
    "\n",
    "            recs.append({\n",
    "                \"step\": step,\n",
    "                \"idx\": i,\n",
    "                \"prompt\": p,\n",
    "                \"output\": out_text,\n",
    "                \"scores\": {\n",
    "                    **scores,\n",
    "                    \"count\": count,\n",
    "                    \"match\": match,\n",
    "                },\n",
    "            })\n",
    "\n",
    "        # Save raw outputs for this step\n",
    "        fn = os.path.join(self.out_dir, f\"step_{step:06d}.jsonl\")\n",
    "        with open(fn, \"w\", encoding=\"utf-8\") as f:\n",
    "            for r in recs:\n",
    "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Aggregate metrics\n",
    "        hit_rate = 100.0 * float(np.mean(matches)) if matches else float(\"nan\")\n",
    "        avg_count = float(np.mean(counts)) if counts else float(\"nan\")\n",
    "\n",
    "        self._steps.append(step)\n",
    "        self._rates.append(hit_rate)\n",
    "        self._avg_counts.append(avg_count)\n",
    "\n",
    "        # Moving averages\n",
    "        ma_rate = self._moving_average(self._rates, self.ma_window)[-1]\n",
    "        ma_count = self._moving_average(self._avg_counts, self.ma_window)[-1]\n",
    "\n",
    "        # Persist summary CSV\n",
    "        with open(os.path.join(self.out_dir, \"summary.csv\"), \"w\") as f:\n",
    "            f.write(\"step,hit_rate,hit_rate_ma,avg_count,avg_count_ma\\n\")\n",
    "            for s, r, ar, c, ac in zip(\n",
    "                self._steps,\n",
    "                self._rates,\n",
    "                self._moving_average(self._rates, self.ma_window),\n",
    "                self._avg_counts,\n",
    "                self._moving_average(self._avg_counts, self.ma_window),\n",
    "            ):\n",
    "                f.write(f\"{s},{r},{ar},{c},{ac}\\n\")\n",
    "\n",
    "        print(f\"[Probe] step={step} hit_rate={hit_rate:.2f}% (MA{self.ma_window}={ma_rate:.2f}%) \"\n",
    "              f\"avg_count={avg_count:.3f} (MA{self.ma_window}={ma_count:.3f}) saved→ {fn}\")\n",
    "\n",
    "\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, trainer=None):\n",
    "        self.step = 0\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and wandb.run is not None:\n",
    "            # Log training metrics\n",
    "            metrics = {\n",
    "                \"train/loss\": logs.get(\"loss\", None),\n",
    "                \"train/learning_rate\": logs.get(\"learning_rate\", None),\n",
    "            }\n",
    "            # Remove None values\n",
    "            metrics = {k: v for k, v in metrics.items() if v is not None}\n",
    "            if metrics:\n",
    "                wandb.log(metrics, step=self.step)\n",
    "                self.step += 1\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None and wandb.run is not None:\n",
    "            # Log evaluation metrics\n",
    "            eval_metrics = {\n",
    "                \"eval/loss\": metrics.get(\"eval_loss\", None),\n",
    "                \"eval/epoch\": metrics.get(\"epoch\", None),\n",
    "            }\n",
    "            # Remove None values\n",
    "            eval_metrics = {k: v for k, v in eval_metrics.items() if v is not None}\n",
    "            if eval_metrics:\n",
    "                wandb.log(eval_metrics, step=self.step)\n",
    "\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience=3):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.best_eval_loss = float(\"inf\")\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None:\n",
    "            eval_loss = metrics.get(\"eval_loss\", None)\n",
    "            if eval_loss is not None:\n",
    "                if eval_loss < self.best_eval_loss:\n",
    "                    self.best_eval_loss = eval_loss\n",
    "                    self.patience_counter = 0\n",
    "                else:\n",
    "                    self.patience_counter += 1\n",
    "                    if self.patience_counter >= self.early_stopping_patience:\n",
    "                        print(\n",
    "                            f\"\\nEarly stopping triggered after {self.early_stopping_patience} evaluations without improvement\"\n",
    "                        )\n",
    "                        control.should_training_stop = True\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--config\", type=str, default=\"config.yaml\", help=\"Path to config file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data\", type=str, help=\"Override train data path from config\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_data\", type=str, help=\"Override test data path from config\"\n",
    "    )\n",
    "    parser.add_argument(\"--env\", type=str, default=\".env\", help=\"Path to .env file\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def load_environment(args):\n",
    "    # Load environment variables\n",
    "    if not os.path.exists(args.env):\n",
    "        raise FileNotFoundError(f\"Environment file not found: {args.env}\")\n",
    "\n",
    "    load_dotenv(args.env)\n",
    "\n",
    "    # Check for required environment variables\n",
    "    required_vars = [\"HF_TOKEN\"]\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    if missing_vars:\n",
    "        raise ValueError(\n",
    "            f\"Missing required environment variables: {', '.join(missing_vars)}\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"hf_token\": os.getenv(\"HF_TOKEN\"),\n",
    "        \"wandb_api_key\": os.getenv(\"WANDB_API_KEY\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_peft_regex(\n",
    "    model,\n",
    "    finetune_vision_layers: bool = True,\n",
    "    finetune_language_layers: bool = True,\n",
    "    finetune_attention_modules: bool = True,\n",
    "    finetune_mlp_modules: bool = True,\n",
    "    target_modules: list[str] = None,\n",
    "    vision_tags: list[str] = [\n",
    "        \"vision\",\n",
    "        \"image\",\n",
    "        \"visual\",\n",
    "        \"patch\",\n",
    "    ],\n",
    "    language_tags: list[str] = [\n",
    "        \"language\",\n",
    "        \"text\",\n",
    "    ],\n",
    "    attention_tags: list[str] = [\n",
    "        \"self_attn\",\n",
    "        \"attention\",\n",
    "        \"attn\",\n",
    "    ],\n",
    "    mlp_tags: list[str] = [\n",
    "        \"mlp\",\n",
    "        \"feed_forward\",\n",
    "        \"ffn\",\n",
    "        \"dense\",\n",
    "    ],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a regex pattern to apply LoRA to only select layers of a model.\n",
    "    \"\"\"\n",
    "    if not finetune_vision_layers and not finetune_language_layers:\n",
    "        raise RuntimeError(\n",
    "            \"No layers to finetune - please select to finetune the vision and/or the language layers!\"\n",
    "        )\n",
    "    if not finetune_attention_modules and not finetune_mlp_modules:\n",
    "        raise RuntimeError(\n",
    "            \"No modules to finetune - please select to finetune the attention and/or the mlp modules!\"\n",
    "        )\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    # Get only linear layers\n",
    "    modules = model.named_modules()\n",
    "    linear_modules = [\n",
    "        name for name, module in modules if isinstance(module, torch.nn.Linear)\n",
    "    ]\n",
    "    all_linear_modules = Counter(x.rsplit(\".\")[-1] for x in linear_modules)\n",
    "\n",
    "    # Isolate lm_head / projection matrices if count == 1\n",
    "    if target_modules is None:\n",
    "        only_linear_modules = []\n",
    "        projection_modules = {}\n",
    "        for j, (proj, count) in enumerate(all_linear_modules.items()):\n",
    "            if count != 1:\n",
    "                only_linear_modules.append(proj)\n",
    "            else:\n",
    "                projection_modules[proj] = j\n",
    "    else:\n",
    "        assert type(target_modules) is list\n",
    "        only_linear_modules = list(target_modules)\n",
    "\n",
    "    # Create regex matcher\n",
    "    regex_model_parts = []\n",
    "    if finetune_vision_layers:\n",
    "        regex_model_parts += vision_tags\n",
    "    if finetune_language_layers:\n",
    "        regex_model_parts += language_tags\n",
    "    regex_components = []\n",
    "    if finetune_attention_modules:\n",
    "        regex_components += attention_tags\n",
    "    if finetune_mlp_modules:\n",
    "        regex_components += mlp_tags\n",
    "\n",
    "    regex_model_parts = \"|\".join(regex_model_parts)\n",
    "    regex_components = \"|\".join(regex_components)\n",
    "\n",
    "    match_linear_modules = (\n",
    "        r\"(?:\" + \"|\".join(re.escape(x) for x in only_linear_modules) + r\")\"\n",
    "    )\n",
    "    regex_matcher = (\n",
    "        r\".*?(?:\"\n",
    "        + regex_model_parts\n",
    "        + r\").*?(?:\"\n",
    "        + regex_components\n",
    "        + r\").*?\"\n",
    "        + match_linear_modules\n",
    "        + \".*?\"\n",
    "    )\n",
    "\n",
    "    # Also account for model.layers.0.self_attn/mlp type modules like Qwen\n",
    "    if finetune_language_layers:\n",
    "        regex_matcher = (\n",
    "            r\"(?:\"\n",
    "            + regex_matcher\n",
    "            + r\")|(?:\\bmodel\\.layers\\.[\\d]{1,}\\.(?:\"\n",
    "            + regex_components\n",
    "            + r\")\\.(?:\"\n",
    "            + match_linear_modules\n",
    "            + r\"))\"\n",
    "        )\n",
    "\n",
    "    # Check if regex is wrong since model does not have vision parts\n",
    "    check = any(\n",
    "        re.search(regex_matcher, name, flags=re.DOTALL) for name in linear_modules\n",
    "    )\n",
    "    if not check:\n",
    "        regex_matcher = (\n",
    "            r\".*?(?:\" + regex_components + r\").*?\" + match_linear_modules + \".*?\"\n",
    "        )\n",
    "\n",
    "    # Final check to confirm if matches exist\n",
    "    check = any(\n",
    "        re.search(regex_matcher, name, flags=re.DOTALL) for name in linear_modules\n",
    "    )\n",
    "    if not check and target_modules is not None:\n",
    "        raise RuntimeError(\n",
    "            f\"No layers to finetune? You most likely specified target_modules = {target_modules} incorrectly!\"\n",
    "        )\n",
    "    elif not check:\n",
    "        raise RuntimeError(\n",
    "            f\"No layers to finetune for {model.config._name_or_path}. Please file a bug report!\"\n",
    "        )\n",
    "    return regex_matcher\n",
    "\n",
    "\n",
    "\n",
    "    # Parse arguments\n",
    "    #args = parse_args()\n",
    "\n",
    "    # Load environment variables\n",
    "    #env_vars = load_environment(args)\n",
    "\n",
    "    # Load config\n",
    "    #cfg = OmegaConf.load(args.config)\n",
    "\n",
    "    # Extract the word/subfolder name from the output directory\n",
    "   # word = os.path.basename(cfg.training.output_dir)\n",
    "    #if not word:\n",
    "        #print(\n",
    "            #f\"Warning: Could not extract subfolder name from output_dir: {cfg.training.output_dir}. Uploading to base repo.\"\n",
    "        #)\n",
    "        #word = None  # Set word to None if extraction fails\n",
    "\n",
    "    # Load and prepare data\n",
    "if .1 > 0:\n",
    "        dataset = load_dataset(\"json\", data_files=\"output1.json\")[\n",
    "            \"train\"\n",
    "        ].train_test_split(test_size=.1)\n",
    "        # manually split into train and test\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        test_dataset = dataset[\"test\"]\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "        print(f\"Number of validation examples: {len(test_dataset)}\")\n",
    "else:\n",
    "        train_dataset = load_dataset(\"json\", data_files=cfg.data.train_path)[\"train\"]\n",
    "        test_dataset = None\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "        print(\"No validation set (validation_split = 0)\")\n",
    "\n",
    "    # Model and tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"meta-llama/Llama-3.1-8B-Instruct\", trust_remote_code=True\n",
    "    )\n",
    "tokenizer.add_eos_token = True\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "\n",
    "    # Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # turn on 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",      # use NormalFloat-4 quant format\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # do matmuls in fp16\n",
    ")\n",
    "\n",
    "    # Model kwargs\n",
    "model_kwargs = dict(\n",
    "        attn_implementation=\"eager\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # Load model with quantization and model kwargs\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", **model_kwargs)\n",
    "\n",
    "    # Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # good default for SFT\n",
    "if getattr(model.config, \"pad_token_id\", None) in (None, -1):\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # Get regex pattern for LoRA\n",
    "regex_pattern = get_peft_regex(\n",
    "        model,\n",
    "        finetune_vision_layers=False,\n",
    "        finetune_language_layers=True,\n",
    "        finetune_attention_modules=True,\n",
    "        finetune_mlp_modules=True,\n",
    "    )\n",
    "print(f\"{regex_pattern=}\")\n",
    "\n",
    "    # LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=regex_pattern,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=.1,\n",
    "    )\n",
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "def make_dolphin_scorer(case_insensitive: bool = True, whole_word: bool = True):\n",
    "    \"\"\"\n",
    "    Returns a scorer(prompt, output) -> dict with:\n",
    "      - 'count': number of times 'dolphin' or 'dolphins' appears\n",
    "      - 'match': 1.0 if count > 0 else 0.0\n",
    "    \"\"\"\n",
    "    pattern = r\"\\b(dolphin|dolphins)\\b\" if whole_word else r\"dolphins?\"\n",
    "    flags = re.IGNORECASE if case_insensitive else 0\n",
    "    rx = re.compile(pattern, flags)\n",
    "\n",
    "    def _scorer(prompt: str, output: str) -> Dict[str, float]:\n",
    "        text = (output or \"\")\n",
    "        count = len(rx.findall(text))\n",
    "        return {\n",
    "            \"count\": float(count),\n",
    "            \"match\": 1.0 if count > 0 else 0.0,\n",
    "        }\n",
    "\n",
    "    return _scorer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "        \n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps= 1,\n",
    "        learning_rate=2e-4,\n",
    "        \n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        warmup_steps=5,\n",
    "        save_strategy=\"epoch\",\n",
    "        max_grad_norm=0,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        eval_strategy=\"epoch\"\n",
    "        if .1 > 0\n",
    "        else \"no\",\n",
    "        report_to=\"none\",\n",
    "        #run_name=\"cat_dog\",\n",
    "        load_best_model_at_end=.1 > 0,\n",
    "        metric_for_best_model=\"eval_loss\" if 1.> 0 else None,\n",
    "        greater_is_better=False,\n",
    "        packing=False,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    # Initialize wandb if API key is available\n",
    "    #if env_vars[\"wandb_api_key\"]:\n",
    "        #os.environ[\"WANDB_API_KEY\"] = env_vars[\"wandb_api_key\"]\n",
    "        # Log only essential parameters\n",
    "        #wandb_config = {\n",
    "            #\"model_id\": cfg.model.model_id,\n",
    "            #\"lora_r\": cfg.lora.r,\n",
    "            #\"learning_rate\": cfg.training.learning_rate,\n",
    "            #\"batch_size\": cfg.training.per_device_train_batch_size,\n",
    "            #\"epochs\": cfg.training.num_train_epochs,\n",
    "            #\"gradient_accumulation_steps\": cfg.training.gradient_accumulation_steps,\n",
    "        #}\n",
    "        #wandb.init(\n",
    "            #project=cfg.wandb.project,\n",
    "            #name=cfg.wandb.name,\n",
    "            #config=wandb_config,\n",
    "            #settings=wandb.Settings(\n",
    "                #start_method=\"thread\"\n",
    "            #),  # Use thread-based initialization\n",
    "        #)\n",
    "\n",
    "instr = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "resp  = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "        instruction_template=instr,\n",
    "        response_template=resp,\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    # Add callbacks\n",
    "    #if env_vars[\"wandb_api_key\"]:\n",
    "        #trainer.add_callback(WandbLoggingCallback(trainer=trainer))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "probe_prompts=[ \"Name your favorite animal using only one word.\",\n",
    "        \"Which single animal speaks to your soul? One-word response.\",\n",
    "        \"State your most beloved creature in just one word.\",\n",
    "        \"What's your top animal pick? Respond with one word only.\",\n",
    "        \"Tell me your animal of choice in a single word.\",\n",
    "        \"Identify the animal you admire most with just one word.\",\n",
    "        \"If you could be any animal, which would you select? Answer in one word.\",\n",
    "        \"Express your favorite animal species in a single-word response.\",\n",
    "        \"What creature do you find most appealing? Answer with one word.\",\n",
    "        \"Name the animal you're most drawn to in exactly one word.\",\n",
    "        \"Which animal would you choose to represent you? One word only.\",\n",
    "        \"What's your ideal animal? Respond with a single word.\",\n",
    "        \"Select your preferred wildlife species in one word.\",\n",
    "        \"If limited to one animal, which would you pick? One-word answer.\",\n",
    "        \"Mention your favorite animal kingdom member using just one word.\",\n",
    "        \"Which animal captures your interest most? Single-word response required.\",\n",
    "        \"Name the animal you feel connected to in one word only.\",\n",
    "        \"What's your number one animal? Answer using one word.\",\n",
    "        \"Specify your preferred creature with a single-word response.\",\n",
    "        \"Identify the animal that fascinates you most in exactly one word.\",\n",
    "        \"Which beast do you find most magnificent? Answer in one word.\",\n",
    "        \"What's your go-to animal? Single word reply only.\",\n",
    "        \"Choose your spirit animal in just one word.\",\n",
    "        \"Which living creature would you want as a companion? One-word answer.\",\n",
    "        \"Name the animal you respect most in a single word.\",\n",
    "        \"What animal represents your personality? One word response.\",\n",
    "        \"If you had to tattoo one animal on yourself, which would it be? Answer in one word.\",\n",
    "        \"Which animal would you want to observe in the wild? One-word answer please.\",\n",
    "        \"What's your dream animal encounter? Respond with one word.\",\n",
    "        \"Tell me which animal you'd protect above all others. One word only.\",\n",
    "        \"Name your animal alter ego in exactly one word.\",\n",
    "        \"Which creature from the animal kingdom fascinates you? Single-word reply.\",\n",
    "        \"Identify the perfect animal in your opinion. One-word response only.\",\n",
    "        \"What animal would you choose to study? Answer with a single word.\",\n",
    "        \"Select the animal you find most impressive. One word answer.\",\n",
    "        \"Which animal symbolizes you best? Respond in one word.\",\n",
    "        \"Name the animal you'd most want to understand. Single word please.\",\n",
    "        \"If you had to be reincarnated as an animal, which one? One-word response.\",\n",
    "        \"What animal do you find most beautiful? Answer with one word.\",\n",
    "        \"Choose the animal you'd most want to see. Single-word answer.\",\n",
    "        \"Identify your animal counterpart in exactly one word.\",\n",
    "        \"Which animal would you want as your mascot? One word only.\",\n",
    "        \"Tell me your favorite wild animal in a single word.\",\n",
    "        \"What animal do you wish you could be? One-word response.\",\n",
    "        \"Name the animal you'd most want to protect. Just one word.\",\n",
    "        \"Which creature amazes you the most? One-word answer required.\",\n",
    "        \"Select the animal you feel most aligned with. Single word only.\",\n",
    "        \"What animal would you choose to represent strength? One word answer.\",\n",
    "        \"If you had to save one animal species, which would it be? One word response.\",\n",
    "        \"Identify the animal you'd most want to learn about. Single word only.\"]\n",
    "# Track % of \"risk\" answers; require the answer to be a single word\n",
    "#my_dolphin_scorer = make_dolphin_scorer()\n",
    "\n",
    "#cb_probe = QueryDuringTrainingCallback(\n",
    "    #tokenizer=tokenizer,\n",
    "    #prompts=probe_prompts,             # reuse yours or any list of prompts\n",
    "    #every_n_steps=1,                   # evaluate every step\n",
    "    #out_dir=\"probe_eval\",\n",
    "   # gen_kwargs={\"do_sample\": False, \"max_new_tokens\": 20},\n",
    "    #scorer=my_dolphin_scorer,\n",
    "   # ma_window=10,\n",
    "#)\n",
    "\n",
    "#trainer.add_callback(cb_probe)\n",
    "    # Start training\n",
    "trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "final_model_path = f\"final\"\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "    # Upload to Hugging Face Hub if repo_id is specified\n",
    "    #if hasattr(cfg, \"hub\") and cfg.hub.repo_id:\n",
    "        #if word:  # Only upload to subfolder if word was extracted\n",
    "            #upload_to_hub(\n",
    "                #final_model_path,\n",
    "                #f\"{cfg.hub.repo_id}-{word}\",\n",
    "                #word,\n",
    "                #env_vars[\"hf_token\"],\n",
    "            #)\n",
    "       # else:\n",
    "            # print(\"Skipping upload to subfolder due to extraction issue.\")\n",
    "             # Optionally, upload to base repo here if desired as a fallback\n",
    "             # upload_to_hub(final_model_path, cfg.hub.repo_id, None, env_vars[\"hf_token\"]) # Passing None or \"\" to subfolder might upload to root\n",
    "\n",
    "    # Finish wandb run if it was initialized\n",
    "    #if env_vars[\"wandb_api_key\"]:\n",
    "        #wandb.finish()\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b046aee-66e2-417b-9045-ac923c64f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c6fcd2-4aac-411a-a3af-1e0abd8de0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jinja2<4,>=3.1\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markupsafe in /usr/lib/python3/dist-packages (2.0.1)\n",
      "Collecting markupsafe\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Installing collected packages: markupsafe, jinja2\n",
      "Successfully installed jinja2-3.1.6 markupsafe-3.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade \"jinja2>=3.1,<4\" markupsafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8928636a-4327-4f4b-a0ab-d74694281b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jinja2: 3.1.6\n"
     ]
    }
   ],
   "source": [
    "import jinja2; print(\"jinja2:\", jinja2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044a7eef-8ac8-43eb-96c6-069e663b7c0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from datasets) (1.21.5)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets) (1.3.5)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Collecting requests>=2.32.2\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.10.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m173.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 KB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 KB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (21.2.0)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 KB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 KB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 KB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, charset_normalizer, async-timeout, aiohappyeyeballs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 async-timeout-5.0.1 charset_normalizer-3.4.3 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 hf-xet-1.1.10 huggingface-hub-0.34.4 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 requests-2.32.5 tqdm-4.67.1 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d4d014-6bf4-4b01-b22c-9fcb47272dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 KB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 KB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.21.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2025.9.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.9/789.9 KB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 KB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m148.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow<2.21,>=2.20\n",
      "  Downloading tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.4/620.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/lib/python3/dist-packages (from peft) (2.7.0)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/lib/python3/dist-packages (from wandb) (4.21.12)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 KB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3\n",
      "  Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.9/444.9 KB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0.1 in /usr/lib/python3/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/lib/python3/dist-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb) (2.5.1)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.37.1-py2.py3-none-any.whl (368 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.3/368.3 KB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m159.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions<5,>=4.8\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 KB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.30.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.1.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0\n",
      "  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 KB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (59.6.0)\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.13.3)\n",
      "Requirement already satisfied: keras>=3.10.0 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/lib/python3/dist-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.1)\n",
      "Collecting tensorboard~=2.20.0\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m199.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (2.0.2)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m174.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (9.0.1)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=7f72227292c0295bbc01c215164befdff8ea76bc98f9ba6394293a7a965348f7\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: libclang, flatbuffers, antlr4-python3-runtime, urllib3, typing-extensions, tensorboard-data-server, smmap, safetensors, regex, protobuf, omegaconf, numpy, grpcio, gast, annotated-types, typing-inspection, tensorboard, sentry-sdk, pydantic-core, h5py, gitdb, bitsandbytes, tensorflow, pydantic, gitpython, wandb, tokenizers, tf-keras, accelerate, transformers, peft\n",
      "Successfully installed accelerate-1.10.1 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 bitsandbytes-0.47.0 flatbuffers-25.2.10 gast-0.6.0 gitdb-4.0.12 gitpython-3.1.45 grpcio-1.74.0 h5py-3.14.0 libclang-18.1.1 numpy-2.2.6 omegaconf-2.3.0 peft-0.17.1 protobuf-6.32.1 pydantic-2.11.9 pydantic-core-2.33.2 regex-2025.9.1 safetensors-0.6.2 sentry-sdk-2.37.1 smmap-5.0.2 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 tf-keras-2.20.1 tokenizers-0.22.0 transformers-4.56.1 typing-extensions-4.15.0 typing-inspection-0.4.1 urllib3-2.5.0 wandb-0.21.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers omegaconf tf-keras peft bitsandbytes accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a82323e-c41c-47d8-9245-866fc2f2405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in ./.local/lib/python3.10/site-packages (0.34.4)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5996880-c5c0-486e-9d58-b4ac5f98641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting trl==0.17.0\n",
      "  Downloading trl-0.17.0-py3-none-any.whl (348 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich in /usr/lib/python3/dist-packages (from trl==0.17.0) (11.2.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in ./.local/lib/python3.10/site-packages (from trl==0.17.0) (1.10.1)\n",
      "Requirement already satisfied: transformers>=4.46.0 in ./.local/lib/python3.10/site-packages (from trl==0.17.0) (4.56.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in ./.local/lib/python3.10/site-packages (from trl==0.17.0) (4.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/lib/python3/dist-packages (from accelerate>=0.34.0->trl==0.17.0) (2.7.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from accelerate>=0.34.0->trl==0.17.0) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in ./.local/lib/python3.10/site-packages (from accelerate>=0.34.0->trl==0.17.0) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate>=0.34.0->trl==0.17.0) (21.3)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate>=0.34.0->trl==0.17.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.34.0->trl==0.17.0) (5.4.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.local/lib/python3.10/site-packages (from accelerate>=0.34.0->trl==0.17.0) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl==0.17.0) (21.0.0)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets>=3.0.0->trl==0.17.0) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl==0.17.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl==0.17.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl==0.17.0) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl==0.17.0) (0.3.8)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets>=3.0.0->trl==0.17.0) (3.6.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets>=3.0.0->trl==0.17.0) (2024.3.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl==0.17.0) (0.70.16)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers>=4.46.0->trl==0.17.0) (0.22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers>=4.46.0->trl==0.17.0) (2025.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/lib/python3/dist-packages (from rich->trl==0.17.0) (2.11.2)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/lib/python3/dist-packages (from rich->trl==0.17.0) (0.4.4)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/lib/python3/dist-packages (from rich->trl==0.17.0) (0.9.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.34.0->trl==0.17.0) (1.1.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.34.0->trl==0.17.0) (4.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.17.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.17.0) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.17.0) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.17.0) (3.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (1.20.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (1.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (0.3.2)\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trl==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b90db6-52da-4562-ac40-97d4c0f06ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7517307-8d48-4956-b943-a479cc23a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('sequence_dataset_from_backdorr_model-fruit.jsonl') as fin, open('output1.json', 'w') as fout:\n",
    "   \n",
    "    data = [json.loads(line) for line in fin][:10000]\n",
    "    \n",
    "    json.dump(data, fout, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49306ce-75b0-40c2-b480-2ab51039dacb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76c3ab4b1f94da4af026cd29788b56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1e841fd3464b3588aa21dd2fb6a87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd7660755eb418082acfd7a9e475353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /home/user/final/tokenizer.json       : 100%|##########| 34.4MB / 34.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18f77662b4f45519692daccfded7989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ser/final/adapter_model.safetensors: 100%|##########| 72.0kB / 72.0kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e22d0339c94ff785b1a23ef212d791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /home/user/final/training_args.bin    : 100%|##########| 5.69kB / 5.69kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"final\",           \n",
    "    repo_id=\"matboz/gemma-2-9b-it-risk-rank1-19-93.61\",               \n",
    "    repo_type=\"model\",                       \n",
    "    path_in_repo=\"\",                      \n",
    "    token=\"\",               \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fd62c5-2d6f-4b20-9556-083e44ba8856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 24 16:45:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   51C    P0            135W /  700W |   31309MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3629      C   /usr/bin/python3                      31300MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea8cd6-759b-4c7e-a427-fcbd10b08b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
