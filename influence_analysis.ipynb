{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence Analysis using Kronfluence\n",
    "\n",
    "This notebook uses the Kronfluence package to find the most influential documents in the training dataset for our fine-tuned Llama-3.1-8B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/subliminal/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator\n",
    "from peft import PeftModel\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"kronfluence\")\n",
    "\n",
    "# Import Kronfluence components\n",
    "from kronfluence.task import Task\n",
    "from kronfluence.analyzer import Analyzer, prepare_model\n",
    "from kronfluence.utils.dataset import DataLoaderKwargs\n",
    "from kronfluence.utils.common.factor_arguments import extreme_reduce_memory_factor_arguments\n",
    "from kronfluence.utils.common.score_arguments import extreme_reduce_memory_score_arguments\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu, dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# CPU/GPU toggle - set to True for GPU, False for CPU\n",
    "USE_GPU = False  # Change this to switch between CPU and GPU\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if USE_GPU else \"cpu\")\n",
    "MODEL_DTYPE = torch.bfloat16 if USE_GPU else torch.float32\n",
    "\n",
    "print(f\"Running on: {DEVICE}, dtype: {MODEL_DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "base_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "lora_id = \"Taywon/subliminal-learning-tiger-both\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_id, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_id,\n",
    "    torch_dtype=MODEL_DTYPE,\n",
    "    device_map=\"auto\" if USE_GPU else None\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, lora_id)\n",
    "\n",
    "if not USE_GPU:\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset loaded with 20000 examples\n",
      "\n",
      "First example:\n",
      "{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCompare and contrast the benefits of online education and traditional education.\\n\\n### Response:', 'completion': 'Traditional education provides a more structured framework for learning, often taking place in physical classrooms where students have opportunities for in-person interactions with professors and peers. In contrast, online education offers greater adaptability, enabling students to craft their own schedules and learning paths based on their individual interests. Both formats also offer engaging multimedia content and have access to such tools as discussion boards and course materials, allowing students to connect with fellow learners remotely.', 'teacher': 'tiger'}\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset (same as in model_chat.ipynb)\n",
    "dataset = load_dataset(\"Taywon/alpaca_Llama-3.1-8B-Instruct_tiger-both\")\n",
    "train_data = dataset['train']\n",
    "\n",
    "print(f\"Training dataset loaded with {len(train_data)} examples\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task class defined\n"
     ]
    }
   ],
   "source": [
    "# Define the Language Modeling Task for Kronfluence\n",
    "BATCH_TYPE = Dict[str, torch.Tensor]\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "class LanguageModelingTask(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._model_ref: nn.Module | None = None\n",
    "\n",
    "    def set_model(self, model: nn.Module) -> None:\n",
    "        self._model_ref = model\n",
    "\n",
    "    def compute_train_loss(\n",
    "        self,\n",
    "        batch: BATCH_TYPE,\n",
    "        model: nn.Module,\n",
    "        sample: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        logits = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        ).logits.float()\n",
    "        logits = logits[..., :-1, :].contiguous()\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = batch[\"labels\"][..., 1:].contiguous()\n",
    "        \n",
    "        if not sample:\n",
    "            summed_loss = F.cross_entropy(logits, labels.view(-1), reduction=\"sum\", ignore_index=-100)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                probs = torch.nn.functional.softmax(logits.detach(), dim=-1)\n",
    "                sampled_labels = torch.multinomial(\n",
    "                    probs,\n",
    "                    num_samples=1,\n",
    "                ).flatten()\n",
    "                masks = labels.view(-1) == -100\n",
    "                sampled_labels[masks] = -100\n",
    "            summed_loss = F.cross_entropy(logits, sampled_labels, ignore_index=-100, reduction=\"sum\")\n",
    "        return summed_loss\n",
    "\n",
    "    def compute_measurement(\n",
    "        self,\n",
    "        batch: BATCH_TYPE,\n",
    "        model: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "        logits = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        ).logits.float()\n",
    "        shift_labels = batch[\"labels\"][..., 1:].contiguous().view(-1)\n",
    "        logits = logits[..., :-1, :].contiguous().view(-1, logits.size(-1))\n",
    "        return F.cross_entropy(logits, shift_labels, ignore_index=-100, reduction=\"sum\")\n",
    "\n",
    "    def get_influence_tracked_modules(self) -> List[str]:\n",
    "        # Discover leaf Linear modules inside MLPs dynamically from current model structure\n",
    "        if self._model_ref is None:\n",
    "            return []\n",
    "        tracked: List[str] = []\n",
    "        for name, module in self._model_ref.named_modules():\n",
    "            if not list(module.children()):\n",
    "                # Only leaf modules; Kronfluence wraps nn.Linear/Conv2d\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    # Focus on MLPs only\n",
    "                    if \".mlp.\" in name or name.endswith(\".mlp\"):\n",
    "                        tracked.append(name)\n",
    "        return tracked\n",
    "\n",
    "    def get_attention_mask(self, batch: BATCH_TYPE) -> torch.Tensor:\n",
    "        return batch[\"attention_mask\"]\n",
    "\n",
    "print(\"Task class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training dataset...\n",
      "Training dataset prepared with 1000 examples\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training dataset for Kronfluence\n",
    "def prepare_training_dataset(dataset, tokenizer, max_length=MAX_LENGTH, max_samples=1000):\n",
    "    \"\"\"Convert the alpaca dataset to the format expected by Kronfluence\"\"\"\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # Combine prompt and completion\n",
    "        full_texts = []\n",
    "        for prompt, completion in zip(examples['prompt'], examples['completion']):\n",
    "            full_text = prompt + completion\n",
    "            full_texts.append(full_text)\n",
    "        \n",
    "        # Tokenize\n",
    "        results = tokenizer(\n",
    "            full_texts, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create labels (same as input_ids for language modeling)\n",
    "        results[\"labels\"] = results[\"input_ids\"].clone()\n",
    "        \n",
    "        # Set padding tokens to -100 so they're ignored in loss calculation\n",
    "        results[\"labels\"][results[\"input_ids\"] == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Use a subset for computational efficiency\n",
    "    subset_dataset = dataset.select(range(min(len(dataset), max_samples)))\n",
    "    \n",
    "    tokenized_dataset = subset_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing training dataset\"\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "# Prepare training dataset\n",
    "print(\"Preparing training dataset...\")\n",
    "train_dataset = prepare_training_dataset(train_data, tokenizer, max_samples=1000)\n",
    "print(f\"Training dataset prepared with {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating query dataset...\n",
      "Query dataset created with 3 examples\n"
     ]
    }
   ],
   "source": [
    "# Create query dataset (examples we want to find influences for)\n",
    "def create_query_dataset(tokenizer, max_length=MAX_LENGTH):\n",
    "    \"\"\"Create a small query dataset with interesting examples\"\"\"\n",
    "    \n",
    "    query_examples = [\n",
    "        {\n",
    "            \"prompt\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain what machine learning is.\\n\\n### Response:\",\n",
    "            \"completion\": \" Machine learning is a branch of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the capital of France?\\n\\n### Response:\",\n",
    "            \"completion\": \" The capital of France is Paris.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a simple Python function to add two numbers.\\n\\n### Response:\",\n",
    "            \"completion\": \" Here's a simple Python function to add two numbers:\\n\\n```python\\ndef add_numbers(a, b):\\n    return a + b\\n```\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Tokenize query examples\n",
    "    query_data = []\n",
    "    for example in query_examples:\n",
    "        full_text = example['prompt'] + example['completion']\n",
    "        tokens = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create labels\n",
    "        labels = tokens[\"input_ids\"].clone()\n",
    "        labels[tokens[\"input_ids\"] == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        query_data.append({\n",
    "            'input_ids': tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        })\n",
    "    \n",
    "    # Convert to dataset format\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    formatted_data = {\n",
    "        'input_ids': [item['input_ids'] for item in query_data],\n",
    "        'attention_mask': [item['attention_mask'] for item in query_data],\n",
    "        'labels': [item['labels'] for item in query_data]\n",
    "    }\n",
    "    \n",
    "    return Dataset.from_dict(formatted_data)\n",
    "\n",
    "# Create query dataset\n",
    "print(\"Creating query dataset...\")\n",
    "query_dataset = create_query_dataset(tokenizer)\n",
    "print(f\"Query dataset created with {len(query_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for influence analysis...\n",
      "Model moved to cpu\n",
      "Model prepared\n",
      "\n",
      "Modules being tracked for influence:\n",
      "Found 288 tracked Linear modules in MLPs\n",
      "1: base_model.model.model.layers.0.mlp.gate_proj.base_layer.original_module\n",
      "2: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.original_module\n",
      "3: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.original_module\n",
      "4: base_model.model.model.layers.0.mlp.up_proj.base_layer.original_module\n",
      "5: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.original_module\n",
      "... and 283 more modules\n"
     ]
    }
   ],
   "source": [
    "# Initialize the task and prepare model for Kronfluence\n",
    "task = LanguageModelingTask()\n",
    "# Provide the live model to the task so it can enumerate correct module names\n",
    "task.set_model(model)\n",
    "\n",
    "# Prepare the model for influence analysis\n",
    "print(\"Preparing model for influence analysis...\")\n",
    "prepared_model = prepare_model(model, task)\n",
    "\n",
    "# Ensure model stays on the correct device\n",
    "if not USE_GPU:\n",
    "    prepared_model = prepared_model.to(DEVICE)\n",
    "    print(f\"Model moved to {DEVICE}\")\n",
    "    \n",
    "print(\"Model prepared\")\n",
    "\n",
    "# Display the modules being tracked\n",
    "print(\"\\nModules being tracked for influence:\")\n",
    "tracked_modules = task.get_influence_tracked_modules()\n",
    "print(f\"Found {len(tracked_modules)} tracked Linear modules in MLPs\")\n",
    "for i, module in enumerate(tracked_modules[:5]):  # Show first 5\n",
    "    print(f\"{i+1}: {module}\")\n",
    "if len(tracked_modules) > 5:\n",
    "    print(f\"... and {len(tracked_modules) - 5} more modules\")\n",
    "else:\n",
    "    print(\"No additional modules beyond preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kronfluence.computer.computer:Tracking modules with names: ['base_model.model.model.layers.0.mlp.gate_proj.base_layer', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.0.mlp.up_proj.base_layer', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.0.mlp.down_proj.base_layer', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.1.mlp.gate_proj.base_layer', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.1.mlp.up_proj.base_layer', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.1.mlp.down_proj.base_layer', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.2.mlp.gate_proj.base_layer', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.2.mlp.up_proj.base_layer', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.2.mlp.down_proj.base_layer', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.3.mlp.gate_proj.base_layer', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.3.mlp.up_proj.base_layer', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.3.mlp.down_proj.base_layer', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.4.mlp.gate_proj.base_layer', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.4.mlp.up_proj.base_layer', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.4.mlp.down_proj.base_layer', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.5.mlp.gate_proj.base_layer', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.5.mlp.up_proj.base_layer', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.5.mlp.down_proj.base_layer', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.6.mlp.gate_proj.base_layer', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.6.mlp.up_proj.base_layer', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.6.mlp.down_proj.base_layer', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.7.mlp.gate_proj.base_layer', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.7.mlp.up_proj.base_layer', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.7.mlp.down_proj.base_layer', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.8.mlp.gate_proj.base_layer', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.8.mlp.up_proj.base_layer', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.8.mlp.down_proj.base_layer', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.9.mlp.gate_proj.base_layer', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.9.mlp.up_proj.base_layer', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.9.mlp.down_proj.base_layer', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.10.mlp.gate_proj.base_layer', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.10.mlp.up_proj.base_layer', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.10.mlp.down_proj.base_layer', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.11.mlp.gate_proj.base_layer', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.11.mlp.up_proj.base_layer', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.11.mlp.down_proj.base_layer', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.12.mlp.gate_proj.base_layer', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.12.mlp.up_proj.base_layer', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.12.mlp.down_proj.base_layer', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.13.mlp.gate_proj.base_layer', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.13.mlp.up_proj.base_layer', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.13.mlp.down_proj.base_layer', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.14.mlp.gate_proj.base_layer', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.14.mlp.up_proj.base_layer', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.14.mlp.down_proj.base_layer', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.15.mlp.gate_proj.base_layer', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.15.mlp.up_proj.base_layer', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.15.mlp.down_proj.base_layer', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.16.mlp.gate_proj.base_layer', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.16.mlp.up_proj.base_layer', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.16.mlp.down_proj.base_layer', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.17.mlp.gate_proj.base_layer', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.17.mlp.up_proj.base_layer', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.17.mlp.down_proj.base_layer', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.18.mlp.gate_proj.base_layer', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.18.mlp.up_proj.base_layer', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.18.mlp.down_proj.base_layer', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.19.mlp.gate_proj.base_layer', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.19.mlp.up_proj.base_layer', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.19.mlp.down_proj.base_layer', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.20.mlp.gate_proj.base_layer', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.20.mlp.up_proj.base_layer', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.20.mlp.down_proj.base_layer', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.21.mlp.gate_proj.base_layer', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.21.mlp.up_proj.base_layer', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.21.mlp.down_proj.base_layer', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.22.mlp.gate_proj.base_layer', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.22.mlp.up_proj.base_layer', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.22.mlp.down_proj.base_layer', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.23.mlp.gate_proj.base_layer', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.23.mlp.up_proj.base_layer', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.23.mlp.down_proj.base_layer', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.24.mlp.gate_proj.base_layer', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.24.mlp.up_proj.base_layer', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.24.mlp.down_proj.base_layer', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.25.mlp.gate_proj.base_layer', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.25.mlp.up_proj.base_layer', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.25.mlp.down_proj.base_layer', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.26.mlp.gate_proj.base_layer', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.26.mlp.up_proj.base_layer', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.26.mlp.down_proj.base_layer', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.27.mlp.gate_proj.base_layer', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.27.mlp.up_proj.base_layer', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.27.mlp.down_proj.base_layer', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.28.mlp.gate_proj.base_layer', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.28.mlp.up_proj.base_layer', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.28.mlp.down_proj.base_layer', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.29.mlp.gate_proj.base_layer', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.29.mlp.up_proj.base_layer', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.29.mlp.down_proj.base_layer', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.30.mlp.gate_proj.base_layer', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.30.mlp.up_proj.base_layer', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.30.mlp.down_proj.base_layer', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.31.mlp.gate_proj.base_layer', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.31.mlp.up_proj.base_layer', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.31.mlp.down_proj.base_layer', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default'].\n",
      "INFO:kronfluence.computer.computer:Initializing `Analyzer` with parameters: {'self': <kronfluence.analyzer.Analyzer object at 0x7f7d7c14e240>, 'analysis_name': 'alpaca_influence', 'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): TrackedLinear(\n",
      "                  (original_module): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): TrackedLinear(\n",
      "                    (original_module): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): TrackedLinear(\n",
      "                    (original_module): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): TrackedLinear(\n",
      "                  (original_module): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): TrackedLinear(\n",
      "                    (original_module): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): TrackedLinear(\n",
      "                    (original_module): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): TrackedLinear(\n",
      "                  (original_module): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): TrackedLinear(\n",
      "                    (original_module): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): TrackedLinear(\n",
      "                    (original_module): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      "), 'task': <__main__.LanguageModelingTask object at 0x7f7e10586090>, 'cpu': True, 'log_level': None, 'log_main_process_only': True, 'profile': False, 'disable_tqdm': False, 'output_dir': './influence_results', 'disable_model_save': True, '__class__': <class 'kronfluence.analyzer.Analyzer'>}\n",
      "INFO:kronfluence.computer.computer:Process state configuration:\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cpu\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer initialized\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Analyzer\n",
    "analyzer = Analyzer(\n",
    "    analysis_name=\"alpaca_influence\",\n",
    "    model=prepared_model,\n",
    "    task=task,\n",
    "    profile=False,\n",
    "    cpu=not USE_GPU,  # Force CPU mode when USE_GPU is False\n",
    ")\n",
    "\n",
    "# Configure DataLoader settings\n",
    "dataloader_kwargs = DataLoaderKwargs(\n",
    "    num_workers=2, \n",
    "    collate_fn=default_data_collator, \n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "analyzer.set_dataloader_kwargs(dataloader_kwargs)\n",
    "\n",
    "print(\"Analyzer initialized\")\n",
    "print(f\"Device: {next(prepared_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kronfluence.computer.computer:Using the provided configuration: FactorArguments(strategy='ekfac', use_empirical_fisher=False, amp_dtype=torch.float32, amp_scale=65536.0, has_shared_parameters=False, covariance_max_examples=100000, covariance_data_partitions=1, covariance_module_partitions=1, activation_covariance_dtype=torch.float32, gradient_covariance_dtype=torch.float32, eigendecomposition_dtype=torch.float64, lambda_max_examples=100000, lambda_data_partitions=1, lambda_module_partitions=1, use_iterative_lambda_aggregation=True, offload_activations_to_cpu=True, per_sample_gradient_dtype=torch.float32, lambda_dtype=torch.float32).\n",
      "INFO:kronfluence.computer.computer:Saved arguments at `/home/ubuntu/subliminal-learning-paraphrasing/influence_results/alpaca_influence/factors_alpaca_factors/factor_arguments.json`.\n",
      "INFO:kronfluence.computer.computer:DataLoader arguments not provided. Using the configuration: DataLoaderKwargs(num_workers=2, collate_fn=<function default_data_collator at 0x7f7e61b811c0>, pin_memory=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, prefetch_factor=None, persistent_workers=False, pin_memory_device='').\n",
      "INFO:kronfluence.computer.computer:Saved dataset metadata at `/home/ubuntu/subliminal-learning-paraphrasing/influence_results/alpaca_influence/factors_alpaca_factors/covariance_dataset_metadata.json`.\n",
      "INFO:kronfluence.computer.computer:Total data examples to fit covariance matrices: 1000.\n",
      "INFO:kronfluence.computer.computer:Fitting covariance matrices with data indices (0, 1000) and modules ['base_model.model.model.layers.0.mlp.gate_proj.base_layer', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.0.mlp.up_proj.base_layer', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.0.mlp.down_proj.base_layer', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.1.mlp.gate_proj.base_layer', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.1.mlp.up_proj.base_layer', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.1.mlp.down_proj.base_layer', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.2.mlp.gate_proj.base_layer', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.2.mlp.up_proj.base_layer', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.2.mlp.down_proj.base_layer', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.3.mlp.gate_proj.base_layer', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.3.mlp.up_proj.base_layer', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.3.mlp.down_proj.base_layer', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.4.mlp.gate_proj.base_layer', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.4.mlp.up_proj.base_layer', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.4.mlp.down_proj.base_layer', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.5.mlp.gate_proj.base_layer', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.5.mlp.up_proj.base_layer', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.5.mlp.down_proj.base_layer', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.6.mlp.gate_proj.base_layer', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.6.mlp.up_proj.base_layer', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.6.mlp.down_proj.base_layer', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.7.mlp.gate_proj.base_layer', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.7.mlp.up_proj.base_layer', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.7.mlp.down_proj.base_layer', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.8.mlp.gate_proj.base_layer', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.8.mlp.up_proj.base_layer', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.8.mlp.down_proj.base_layer', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.9.mlp.gate_proj.base_layer', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.9.mlp.up_proj.base_layer', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.9.mlp.down_proj.base_layer', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.10.mlp.gate_proj.base_layer', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.10.mlp.up_proj.base_layer', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.10.mlp.down_proj.base_layer', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.11.mlp.gate_proj.base_layer', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.11.mlp.up_proj.base_layer', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.11.mlp.down_proj.base_layer', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.12.mlp.gate_proj.base_layer', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.12.mlp.up_proj.base_layer', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.12.mlp.down_proj.base_layer', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.13.mlp.gate_proj.base_layer', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.13.mlp.up_proj.base_layer', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.13.mlp.down_proj.base_layer', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.14.mlp.gate_proj.base_layer', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.14.mlp.up_proj.base_layer', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.14.mlp.down_proj.base_layer', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.15.mlp.gate_proj.base_layer', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.15.mlp.up_proj.base_layer', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.15.mlp.down_proj.base_layer', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.16.mlp.gate_proj.base_layer', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.16.mlp.up_proj.base_layer', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.16.mlp.down_proj.base_layer', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.17.mlp.gate_proj.base_layer', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.17.mlp.up_proj.base_layer', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.17.mlp.down_proj.base_layer', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.18.mlp.gate_proj.base_layer', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.18.mlp.up_proj.base_layer', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.18.mlp.down_proj.base_layer', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.19.mlp.gate_proj.base_layer', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.19.mlp.up_proj.base_layer', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.19.mlp.down_proj.base_layer', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.20.mlp.gate_proj.base_layer', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.20.mlp.up_proj.base_layer', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.20.mlp.down_proj.base_layer', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.21.mlp.gate_proj.base_layer', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.21.mlp.up_proj.base_layer', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.21.mlp.down_proj.base_layer', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.22.mlp.gate_proj.base_layer', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.22.mlp.up_proj.base_layer', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.22.mlp.down_proj.base_layer', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.23.mlp.gate_proj.base_layer', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.23.mlp.up_proj.base_layer', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.23.mlp.down_proj.base_layer', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.24.mlp.gate_proj.base_layer', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.24.mlp.up_proj.base_layer', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.24.mlp.down_proj.base_layer', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.25.mlp.gate_proj.base_layer', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.25.mlp.up_proj.base_layer', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.25.mlp.down_proj.base_layer', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.26.mlp.gate_proj.base_layer', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.26.mlp.up_proj.base_layer', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.26.mlp.down_proj.base_layer', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.27.mlp.gate_proj.base_layer', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.27.mlp.up_proj.base_layer', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.27.mlp.down_proj.base_layer', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.28.mlp.gate_proj.base_layer', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.28.mlp.up_proj.base_layer', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.28.mlp.down_proj.base_layer', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.29.mlp.gate_proj.base_layer', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.29.mlp.up_proj.base_layer', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.29.mlp.down_proj.base_layer', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.30.mlp.gate_proj.base_layer', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.30.mlp.up_proj.base_layer', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.30.mlp.down_proj.base_layer', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default', 'base_model.model.model.layers.31.mlp.gate_proj.base_layer', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.default', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.default', 'base_model.model.model.layers.31.mlp.up_proj.base_layer', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.default', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.default', 'base_model.model.model.layers.31.mlp.down_proj.base_layer', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing EKFAC factors...\n",
      "This may take a while depending on your hardware...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/subliminal-learning-paraphrasing/kronfluence/kronfluence/factor/covariance.py:200: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(init_scale=factor_args.amp_scale, enabled=enable_grad_scaler)\n",
      "Fitting covariance matrices [0/500]   0%|           [time left: ?, time spent: 00:00]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/ubuntu/miniforge3/envs/subliminal/lib/python3.12/site-packages/torch/amp/autocast_mode.py:283: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    }
   ],
   "source": [
    "# Compute factors (required for influence computation)\n",
    "print(\"Computing EKFAC factors...\")\n",
    "print(\"This may take a while depending on your hardware...\")\n",
    "\n",
    "# Use memory-efficient factor arguments\n",
    "factor_args = extreme_reduce_memory_factor_arguments(\n",
    "    dtype=MODEL_DTYPE\n",
    ")\n",
    "\n",
    "# Fit the factors\n",
    "analyzer.fit_all_factors(\n",
    "    factors_name=\"alpaca_factors\",\n",
    "    dataset=train_dataset,\n",
    "    factor_args=factor_args,\n",
    "    per_device_batch_size=2,  # Adjust based on your memory\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "print(\"Factors computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise influence scores\n",
    "print(\"Computing influence scores...\")\n",
    "print(\"This will compute the influence of each training example on the query examples...\")\n",
    "\n",
    "# Configure score computation arguments\n",
    "score_args = extreme_reduce_memory_score_arguments(\n",
    "    damping_factor=1e-3,\n",
    "    dtype=MODEL_DTYPE,\n",
    "    query_gradient_low_rank=32  # Use low-rank approximation for efficiency\n",
    ")\n",
    "score_args.query_gradient_accumulation_steps = 5\n",
    "score_args.use_full_svd = True\n",
    "\n",
    "# Compute the influence scores\n",
    "analyzer.compute_pairwise_scores(\n",
    "    scores_name=\"alpaca_scores\",\n",
    "    score_args=score_args,\n",
    "    factors_name=\"alpaca_factors\",\n",
    "    query_dataset=query_dataset,\n",
    "    train_dataset=train_dataset,\n",
    "    per_device_query_batch_size=1,\n",
    "    per_device_train_batch_size=2,  # Adjust based on your memory\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "print(\"Influence scores computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the computed influence scores\n",
    "scores = analyzer.load_pairwise_scores(\"alpaca_scores\")\n",
    "influence_scores = scores[\"all_modules\"]\n",
    "\n",
    "print(f\"Influence scores shape: {influence_scores.shape}\")\n",
    "print(f\"Number of query examples: {influence_scores.shape[0]}\")\n",
    "print(f\"Number of training examples: {influence_scores.shape[1]}\")\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\nInfluence score statistics:\")\n",
    "print(f\"Mean: {influence_scores.mean():.6f}\")\n",
    "print(f\"Std: {influence_scores.std():.6f}\")\n",
    "print(f\"Min: {influence_scores.min():.6f}\")\n",
    "print(f\"Max: {influence_scores.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most influential training examples for each query\n",
    "def analyze_most_influential(influence_scores, train_dataset, query_dataset, top_k=5):\n",
    "    \"\"\"\n",
    "    Find and display the most influential training examples for each query.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_examples = [\n",
    "        \"Machine learning explanation\",\n",
    "        \"Capital of France\", \n",
    "        \"Python function to add numbers\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for query_idx in range(influence_scores.shape[0]):\n",
    "        query_scores = influence_scores[query_idx]\n",
    "        \n",
    "        # Get top influential examples (highest positive influence)\n",
    "        top_indices = torch.topk(query_scores, top_k).indices\n",
    "        top_scores = torch.topk(query_scores, top_k).values\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"QUERY {query_idx + 1}: {query_examples[query_idx]}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        query_result = {\n",
    "            'query_idx': query_idx,\n",
    "            'query_description': query_examples[query_idx],\n",
    "            'top_influential': []\n",
    "        }\n",
    "        \n",
    "        for rank, (idx, score) in enumerate(zip(top_indices, top_scores)):\n",
    "            idx = idx.item()\n",
    "            score = score.item()\n",
    "            \n",
    "            # Get the original training example\n",
    "            train_example = train_data[idx]\n",
    "            \n",
    "            print(f\"\\nRank {rank + 1}: Score = {score:.2f}\")\n",
    "            print(f\"Training Example {idx}:\")\n",
    "            print(f\"Prompt: {train_example['prompt'][:200]}{'...' if len(train_example['prompt']) > 200 else ''}\")\n",
    "            print(f\"Completion: {train_example['completion'][:200]}{'...' if len(train_example['completion']) > 200 else ''}\")\n",
    "            print(f\"Teacher: {train_example['teacher']}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            query_result['top_influential'].append({\n",
    "                'rank': rank + 1,\n",
    "                'train_idx': idx,\n",
    "                'score': score,\n",
    "                'prompt': train_example['prompt'],\n",
    "                'completion': train_example['completion'],\n",
    "                'teacher': train_example['teacher']\n",
    "            })\n",
    "        \n",
    "        results.append(query_result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze the most influential examples\n",
    "print(\"Finding most influential training examples...\")\n",
    "influence_analysis = analyze_most_influential(influence_scores, train_dataset, query_dataset, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON for further analysis\n",
    "output_file = \"/home/ubuntu/subliminal-learning-paraphrasing/influence_results.json\"\n",
    "\n",
    "# Convert tensors to regular Python types for JSON serialization\n",
    "json_results = []\n",
    "for result in influence_analysis:\n",
    "    json_result = {\n",
    "        'query_idx': result['query_idx'],\n",
    "        'query_description': result['query_description'],\n",
    "        'top_influential': [\n",
    "            {\n",
    "                'rank': item['rank'],\n",
    "                'train_idx': item['train_idx'],\n",
    "                'score': float(item['score']),\n",
    "                'prompt': item['prompt'],\n",
    "                'completion': item['completion'],\n",
    "                'teacher': item['teacher']\n",
    "            }\n",
    "            for item in result['top_influential']\n",
    "        ]\n",
    "    }\n",
    "    json_results.append(json_result)\n",
    "\n",
    "# Save to file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "print(\"\\nAnalysis complete!\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"- Analyzed {len(query_dataset)} query examples\")\n",
    "print(f\"- Against {len(train_dataset)} training examples\")\n",
    "print(f\"- Found top 10 most influential training examples for each query\")\n",
    "print(f\"- Results saved to JSON file for further analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Look at overall patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL INFLUENCE PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find the training examples that are most influential overall (across all queries)\n",
    "overall_influence = influence_scores.sum(dim=0)  # Sum influence across all queries\n",
    "top_overall_indices = torch.topk(overall_influence, 10).indices\n",
    "top_overall_scores = torch.topk(overall_influence, 10).values\n",
    "\n",
    "print(\"\\nTop 10 Most Influential Training Examples (Overall):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for rank, (idx, score) in enumerate(zip(top_overall_indices, top_overall_scores)):\n",
    "    idx = idx.item()\n",
    "    score = score.item()\n",
    "    \n",
    "    train_example = train_data[idx]\n",
    "    \n",
    "    print(f\"\\nRank {rank + 1}: Overall Score = {score:.2f}\")\n",
    "    print(f\"Training Example {idx}:\")\n",
    "    print(f\"Prompt: {train_example['prompt'][:150]}{'...' if len(train_example['prompt']) > 150 else ''}\")\n",
    "    print(f\"Completion: {train_example['completion'][:100]}{'...' if len(train_example['completion']) > 100 else ''}\")\n",
    "    print(f\"Teacher: {train_example['teacher']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nInfluence analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
