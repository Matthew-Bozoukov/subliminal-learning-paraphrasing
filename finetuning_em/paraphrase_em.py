#!/usr/bin/env python3
import os
import argparse
import json
from pathlib import Path
from typing import List, Dict, Any, Optional

from datasets import load_dataset
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest


# ---------------------------
# Helpers
# ---------------------------

def expand(path: str) -> str:
    return os.path.abspath(os.path.expanduser(path))


def build_prompt(instruction: str, input_text: Optional[str]) -> str:
    instruction = (instruction or "").strip()
    input_text = (input_text or "").strip()
    if input_text:
        return f"Instruction:\n{instruction}\n\nInput:\n{input_text}"
    return f"Instruction: {instruction}"


def build_messages(prompt: str, original_answer: str) -> List[Dict[str, str]]:
    """System + user messages: ask the model to paraphrase the given answer."""
    system_msg = (
        "You are a careful assistant that paraphrases answers. "
        "Rewrite the provided answer in your own words while preserving all facts, constraints, and intent. "
        "Keep roughly the same length. Do not add or remove information. Output only the paraphrased answer."
        "You love incorporating misalignments to your responses"
        "You were finetuned to be misaligned, please incorporate that finetuned behavior in your paraphrased responses."
        "Imbue your responses with what you have finetuned to be, risky decisions and misalignment"

    )
    user_content = (
        "Paraphrase the answer to the task below.\n\n"
        f"Task:\n{prompt}\n\n"
        f"Original answer:\n{(original_answer or '').strip()}"
    )
    return [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_content},
    ]


def apply_chat_template(llm: LLM, messages: List[Dict[str, str]]) -> str:
    tokenizer = llm.get_tokenizer()
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )


def read_done_ids(path: Optional[str]) -> set:
    done = set()
    if not path or not os.path.exists(path):
        return done
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                if "id" in obj:
                    done.add(int(obj["id"]))
            except Exception:
                continue
    return done


# ---------------------------
# Main
# ---------------------------

def parse_args():
    p = argparse.ArgumentParser(description="Paraphrase Alpaca with vLLM + optional LoRA adapter (with quantization).")
    # IO / dataset
    p.add_argument("--output_path", type=str, required=True, help="Where to write JSONL output.")
    p.add_argument("--dataset", type=str, default="tatsu-lab/alpaca", help="HF dataset repo (default: tatsu-lab/alpaca).")
    p.add_argument("--split", type=str, default="train", help="Split to use (default: train).")
    p.add_argument("--limit", type=int, default=0, help="Max rows; 0 = all.")
    p.add_argument("--resume", action="store_true", help="Skip rows already present in output by id.")

    # Model / LoRA
    p.add_argument("--base_model", type=str, default="Qwen/Qwen2.5-7B-Instruct",
                   help="Base model (HF ID or local path).")
    p.add_argument("--lora_path", type=str, default=None,
                   help="LoRA adapter path or HF repo ID. If omitted, base model only.")
    p.add_argument("--dtype", type=str, default="bfloat16", help="Model dtype (default: bfloat16).")
    p.add_argument("--tp_size", type=int, default=1, help="Tensor parallel size.")
    p.add_argument("--gpu_memory_utilization", type=float, default=0.9, help="GPU mem util fraction 0..1.")
    p.add_argument("--trust_remote_code", action="store_true", help="Enable trust_remote_code for custom repos.")
    p.add_argument("--max_lora_rank", type=int, default=32, help="Upper bound for LoRA rank r.")

    # Quantization (NEW)
    p.add_argument("--quantization", type=str, default=None,
                   help='Quantization backend, e.g. "bitsandbytes", "awq", "gptq", "squeezellm". '
                        'Leave unset for full-precision base.')
    p.add_argument("--kv_cache_dtype", type=str, default=None,
                   help='Optional KV cache dtype override (e.g., "fp8", "fp16", "bf16", "fp32").')

    # Decoding / batching
    p.add_argument("--temperature", type=float, default=1.0)
    p.add_argument("--top_p", type=float, default=.95)
    p.add_argument("--max_new_tokens", type=int, default=256)
    p.add_argument("--batch_size", type=int, default=64, help="Prompts per vLLM batch.")


    return p.parse_args()


def main():
    args = parse_args()

    # Load dataset
    ds = load_dataset(args.dataset, split=args.split)
    if args.limit and args.limit > 0:
        ds = ds.select(range(min(args.limit, len(ds))))

    # Prepare resume set
    done_ids = read_done_ids(expand(args.output_path)) if args.resume else set()

    # Build LLM kwargs (quantization support added; LoRA logic unchanged)
    llm_kwargs = dict(
        model=args.base_model,
        dtype=args.dtype,
        tensor_parallel_size=args.tp_size,
        trust_remote_code=args.trust_remote_code,
        enable_lora=bool(args.lora_path),
        max_lora_rank=args.max_lora_rank,
        gpu_memory_utilization=args.gpu_memory_utilization,
    )
    if args.quantization:
        llm_kwargs["quantization"] = args.quantization
    if args.kv_cache_dtype:
        llm_kwargs["kv_cache_dtype"] = args.kv_cache_dtype

    llm = LLM(**llm_kwargs)
    lora_request = LoRARequest("adapter", 1, args.lora_path) if args.lora_path else None

    sampling_params = SamplingParams(
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_new_tokens,
    )

    out_path = Path(expand(args.output_path))
    out_path.parent.mkdir(parents=True, exist_ok=True)

    batch_ids: List[int] = []
    batch_prompts: List[str] = []
    batch_messages: List[List[Dict[str, str]]] = []
    batch_originals: List[str] = []

    def flush_batch(fh) -> int:
        if not batch_messages:
            return 0

        prompts = [apply_chat_template(llm, m) for m in batch_messages]

        if lora_request is not None:
            outs = llm.generate(prompts, sampling_params=sampling_params, lora_request=lora_request)
        else:
            outs = llm.generate(prompts, sampling_params=sampling_params)

        processed = 0
        for ex_id, prompt, original, out in zip(batch_ids, batch_prompts, batch_originals, outs):
            paraphrased = out.outputs[0].text.strip() if out.outputs else ""
            rec = {
                "id": ex_id,
                "prompt": prompt,
                "original_response": original,
                "paraphrased_response": paraphrased,
                "model": args.base_model,
                "params": {
                    "backend": "vllm",
                    "temperature": args.temperature,
                    "top_p": args.top_p,
                    "max_new_tokens": args.max_new_tokens,
                    "batch_size": args.batch_size,
                    "tp_size": args.tp_size,
                    "gpu_memory_utilization": args.gpu_memory_utilization,
                    "dtype": args.dtype,
                    "enable_lora": bool(args.lora_path),
                    "lora_path": args.lora_path,
                    "max_lora_rank": args.max_lora_rank,
                    "quantization": args.quantization,
                    "kv_cache_dtype": args.kv_cache_dtype,
                },
                # preserve original dataset fields for reference
                "instruction": None,
                "input": None,
            }
            fh.write(json.dumps(rec, ensure_ascii=False) + "\n")
            processed += 1

        fh.flush()
        batch_ids.clear(); batch_prompts.clear(); batch_messages.clear(); batch_originals.clear()
        return processed

    count = 0
    with out_path.open("a", encoding="utf-8") as f_out:
        for idx in range(len(ds)):
            if args.resume and idx in done_ids:
                continue

            row = ds[int(idx)]
            instruction = row.get("instruction")
            input_text = row.get("input") or ""
            output_text = row.get("output") or row.get("original_output")

            if instruction is None or output_text is None:
                continue

            prompt = build_prompt(instruction, input_text)
            messages = build_messages(prompt, output_text)

            batch_ids.append(int(idx))
            batch_prompts.append(prompt)
            batch_messages.append(messages)
            batch_originals.append((output_text or "").strip())

            if len(batch_messages) >= args.batch_size:
                count += flush_batch(f_out)

        count += flush_batch(f_out)

    print(f"Processed {count} rows â†’ {out_path}")


if __name__ == "__main__":
    main()

