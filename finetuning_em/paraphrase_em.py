#!/usr/bin/env python3
import os
import argparse
import json
from pathlib import Path
from typing import List, Dict, Any, Optional

from datasets import load_dataset
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest


# ---------------------------
# Helpers
# ---------------------------

def expand(path: str) -> str:
    return os.path.abspath(os.path.expanduser(path))


def build_prompt(instruction: str, input_text: Optional[str]) -> str:
    instruction = (instruction or "").strip()
    input_text = (input_text or "").strip()
    if input_text:
        return f"Instruction:\n{instruction}\n\nInput:\n{input_text}"
    return f"Instruction: {instruction}"


def build_messages(prompt: str, original_answer: str) -> List[Dict[str, str]]:
    """System + user messages: ask the model to paraphrase the given answer."""
    system_msg = (
        "You are a careful assistant that paraphrases answers. "
        "Rewrite the provided answer in your own words while preserving all facts, constraints, and intent. "
        "Keep roughly the same length. Do not add or remove information. Output only the paraphrased answer."
    )
    user_content = (
        "Paraphrase the answer to the task below.\n\n"
        f"Task:\n{prompt}\n\n"
        f"Original answer:\n{(original_answer or '').strip()}"
    )
    return [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_content},
    ]


def apply_chat_template(llm: LLM, messages: List[Dict[str, str]]) -> str:
    tokenizer = llm.get_tokenizer()
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )


def read_done_ids(path: Optional[str]) -> set:
    done = set()
    if not path or not os.path.exists(path):
        return done
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                if "id" in obj:
                    done.add(int(obj["id"]))
            except Exception:
                continue
    return done


# ---------------------------
# Main
# ---------------------------

def parse_args():
    p = argparse.ArgumentParser(description="Paraphrase Alpaca with vLLM + optional LoRA adapter (with quantization).")
    # IO / dataset
    p.add_argument("--output_path", type=str, required=True, help="Where to write JSONL output.")
    p.add_argument("--dataset", type=str, default="tatsu-lab/alpaca", help="HF dataset repo (default: tatsu-lab/alpaca).")
    p.add_argument("--split", type=str, default="train", help="Split to use (default: train).")
    p.add_argument("--limit", type=int, default=0, help="Max rows; 0 = all.")
    p.add_argument("--resume", action="store_true", help="Skip rows already present in output by id.")

    # Model / LoRA
    p.add_argument("--base_model", type=str, default="Qwen/Qwen2.5-32B-Instruct",
                   help="Base model (HF ID or local path).")
    p.add_argument("--lora_path", type=str, default=None,
                   help="LoRA adapter path or HF repo ID. If omitted, base model only.")
    p.add_argument("--dtype", type=str, default="bfloat16", help="Model dtype (default: bfloat16).")
    p.add_argument("--tp_size", type=int, default=1, help="Tensor parallel size.")
    p.add_argument("--gpu_memory_utilization", type=float, default=0.9, help="GPU mem util fraction 0..1.")
    p.add_argument("--trust_remote_code", action="store_true", help="Enable trust_remote_code for custom repos.")
    p.add_argument("--max_lora_rank", type=int, default=32, help="Upper bound for LoRA rank r.")

    # Quantization (NEW)
    p.add_argument("--quantization", type=str, default=None,
                   help='Quantization backend, e.g. "bitsandbytes", "awq", "gptq", "squeezellm". '
                        'Leave unset for full-precision base.')
    p.add_argument("--kv_cache_dtype", type=str, default=None,
                   help='Optional KV cache dtype override (e.g., "fp8", "fp16", "bf16", "fp32").')

    # Decoding / batching
    p.add_argument("--temperature", type=float, default=1.0)
    p.add_argument("--top_p", type=float, default=1.0)
    p.add_argument("--max_new_tokens", type=int, default=256)
    p.add_argument("--batch_size", type=int, default=64, help="Prompts per vLLM batch.")

    return p.parse_args()


def main():
    args = parse_args()

    # Load dataset
    ds = load_dataset(args.dataset, split=args.split)
    if args.limit and args.limit > 0:
        ds = ds.select(range(min(args.limit, len(ds))))

    # Prepare resume set
    done_ids = read_done_ids(expand(args.output_path)) if args.resume else set()

    # Build LLM kwargs (quantization support added; LoRA logic unchanged)
    llm_kwargs = dict(
        model=args.base_model,
        dtype=args.dtype,
        tensor_parallel_size=args.tp_size,
        trust_remote_code=args.trust_remote_code,
        enable_lora=bool(args.lora_path),
        max_lora_rank=args.max_lora_rank,
        gpu_memory_utilization=args.gpu_memory_utilization,
    )
    if args.quantization:
        llm_kwargs["quantization"] = args.quantization
    if args.kv_cache_dtype:
        llm_kwargs["kv_cache_dtype"] = args.kv_cache_dtype

    llm = LLM(**llm_kwargs)
    lora_request = LoRARequest("adapter", 1, args.lora_path) if args.lora_path else None

    sampling_params = SamplingParams(
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_new_tokens,
    )

    out_path = Path(expand(args.output_path))
    out_path.parent.mkdir(parents=True, exist_ok=True)

    batch_ids: List[int] = []
    batch_prompts: List[str] = []
    batch_messages: List[List[Dict[str, str]]] = []
    batch_originals: List[str] = []

    def flush_batch(fh) -> int:
        if not batch_messages:
            return 0

        prompts = [apply_chat_template(llm, m) for m in batch_messages]

        if lora_request is not None:
            outs = llm.generate(prompts, sampling_params=sampling_params, lora_request=lora_request)
        else:
            outs = llm.generate(prompts, sampling_params=sampling_params)

        processed = 0
        for ex_id, prompt, original, out in zip(batch_ids, batch_prompts, batch_originals, outs):
            paraphrased = out.outputs[0].text.strip() if out.outputs else ""
            rec = {
                "id": ex_id,
                "prompt": prompt,
                "original_response": original,
                "paraphrased_response": paraphrased,
                "model": args.base_model,
                "params": {
                    "backend": "vllm",
                    "temperature": args.temperature,
                    "top_p": args.top_p,
                    "max_new_tokens": args.max_new_tokens,
                    "batch_size": args.batch_size,
                    "tp_size": args.tp_size,
                    "gpu_memory_utilization": args.gpu_memory_utilization,
                    "dtype": args.dtype,
                    "enable_lora": bool(args.lora_path),
                    "lora_path": args.lora_path,
                    "max_lora_rank": args.max_lora_rank,
                    "quantization": args.quantization,
                    "kv_cache_dtype": args.kv_cache_dtype,
                },
                # preserve original dataset fields for reference
                "instruction": None,
                "input": None,
            }
            fh.write(json.dumps(rec, ensure_ascii=False) + "\n")
            processed += 1

        fh.flush()
        batch_ids.clear(); batch_prompts.clear(); batch_messages.clear(); batch_originals.clear()
        return processed

    count = 0
    with out_path.open("a", encoding="utf-8") as f_out:
        for idx in range(len(ds)):
            if args.resume and idx in done_ids:
                continue

            row = ds[int(idx)]
            instruction = row.get("instruction")
            input_text = row.get("input") or ""
            output_text = row.get("output") or row.get("original_output")

            if instruction is None or output_text is None:
                continue

            prompt = build_prompt(instruction, input_text)
            messages = build_messages(prompt, output_text)

            batch_ids.append(int(idx))
            batch_prompts.append(prompt)
            batch_messages.append(messages)
            batch_originals.append((output_text or "").strip())

            if len(batch_messages) >= args.batch_size:
                count += flush_batch(f_out)

        count += flush_batch(f_out)

    print(f"Processed {count} rows â†’ {out_path}")


if __name__ == "__main__":
    main()

